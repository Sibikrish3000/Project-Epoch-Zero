import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.autograd import grad
import matplotlib.pyplot as plt
from spacetrack import SpaceTrackClient
from sgp4.api import Satrec, WGS72
import json
from datetime import datetime, timezone

# Physical constants
MU_EARTH = 398600.4418  # km^3 / s^2
R_EARTH = 6378.137     # km
J2 = 1.08262668e-3
OMEGA_E = 7.2921159e-5  # rad/s

# Normalization Constants (Nondimensionalization)
R_REF = R_EARTH
V_REF = np.sqrt(MU_EARTH / R_EARTH) # ~7.905 km/s
T_REF = R_REF / V_REF               # ~806.8 s
ACC_REF = MU_EARTH / (R_EARTH**2)   # ~0.0098 km/s^2 (1g)


def download_historical_tles(norad_id, start_date, end_date, username, password):
    """
    Downloads TLE history for a specific NORAD ID.
    
    Args:
        norad_id (int): Satellite Catalog Number (e.g., 50803)
        start_date (str): 'YYYY-MM-DD'
        end_date (str): 'YYYY-MM-DD'
        username (str): Space-Track login email
        password (str): Space-Track login password
    """
    print(f"[-] Authenticating as {username}...")
    st = SpaceTrackClient(identity=username, password=password)

    print(f"[-] Fetching TLEs for ID {norad_id} from {start_date} to {end_date}...")
    
    try:
        # Query: Filter by NORAD ID and Epoch Range
        # orderby='EPOCH desc' ensures we get data in order
        tles = st.tle(
            norad_cat_id=norad_id,
            epoch=f"{start_date}--{end_date}",
            orderby='EPOCH',
            format='json'
        )
        
        if not tles:
            print("No TLEs found for this range.")
            return

        # Convert to Pandas DataFrame
        df = pd.DataFrame(json.loads(tles))
        
        # Select critical columns for SGP4 and PINN training
        cols = [
            'EPOCH', 'NORAD_CAT_ID', 'MEAN_MOTION', 'ECCENTRICITY', 
            'INCLINATION', 'RA_OF_ASC_NODE', 'ARG_OF_PERICENTER', 
            'MEAN_ANOMALY', 'MEAN_MOTION_DOT', 'BSTAR', 
            'TLE_LINE1', 'TLE_LINE2'
        ]
        df = df[cols]
        
        # Type cleaning
        df['EPOCH'] = pd.to_datetime(df['EPOCH'])
        df = df.sort_values('EPOCH')
        
        # Save
        filename = f"history_{norad_id}_{start_date}_{end_date}.csv"
        df.to_csv(filename, index=False)
        print(f"[+] Success. Saved {len(df)} TLEs to {filename}")
        print(df[['EPOCH', 'MEAN_MOTION', 'MEAN_MOTION_DOT']].head())
        
    except Exception as e:
        print(f"[!] API Error: {e}")




# Configuration
TARGET_ID = 50803  # Starlink-3321
START = "2022-01-01"
END = "2025-12-17"
username = 'sibikrish3000@gmail.com'
password = 'Z!2V0XBU1arMv8q' 

download_historical_tles(TARGET_ID, START, END, username, password)


def calculate_residuals(csv_path, lookahead_hours=24):
    """
    Generates training pairs (Input State -> Output Error).
    
    Args:
        csv_path (str): Path to the historical TLE CSV.
        lookahead_hours (float): How far into the future to compare (the gap).
    """
    print(f"[-] Loading {csv_path}...")
    df = pd.read_csv(csv_path)
    df['EPOCH'] = pd.to_datetime(df['EPOCH'])
    df = df.sort_values('EPOCH').reset_index(drop=True)
    
    training_data = []
    
    print(f"[-] Generating residuals with {lookahead_hours}h lookahead...")
    
    # Iterate through the timeline
    for i in range(len(df)):
        # 1. The "Anchor" TLE (Input)
        row_start = df.iloc[i]
        sat_start = Satrec.twoline2rv(row_start['TLE_LINE1'], row_start['TLE_LINE2'], WGS72)
        
        # 2. Find a "Target" TLE close to the lookahead time
        # We search for a TLE that was created roughly 'lookahead_hours' later
        target_time = row_start['EPOCH'] + pd.Timedelta(hours=lookahead_hours)
        
        # Filter for rows close to target_time (within +/- 2 hours tolerance)
        # Using searchsorted for speed on sorted index is faster, but simple masking is clearer here
        mask = (df['EPOCH'] > target_time - pd.Timedelta(hours=2)) & \
               (df['EPOCH'] < target_time + pd.Timedelta(hours=2))
        
        candidates = df[mask]
        
        if candidates.empty:
            continue
            
        # Pick the closest candidate
        row_end = candidates.iloc[0]
        sat_end = Satrec.twoline2rv(row_end['TLE_LINE1'], row_end['TLE_LINE2'], WGS72)
        
        # 3. Propagate "Anchor" to "Target" time
        # Calculate time difference in minutes
        time_diff_min = (row_end['EPOCH'] - row_start['EPOCH']).total_seconds() / 60.0
        
        # SGP4 Propagation (Prediction)
        e, r_pred, v_pred = sat_start.sgp4_tsince(time_diff_min)
        
        if e != 0: continue # Skip errors
            
        # 4. Get "Target" True State at its own Epoch
        # We propagate the Target TLE by 0 minutes to get its initial state
        _, r_true, v_true = sat_end.sgp4_tsince(0.0)
        
        # 5. Compute Residuals (The "Label" for the Neural Net)
        # Difference in km and km/s
        diff_r = np.array(r_true) - np.array(r_pred)
        diff_v = np.array(v_true) - np.array(v_pred)
        
        # 6. Store Feature Vector
        # Features: [rx, ry, rz, vx, vy, vz, BSTAR, ndot, time_delta]
        # Targets:  [drx, dry, drz, dvx, dvy, dvz]
        training_data.append({
            'start_epoch': row_start['EPOCH'],
            'end_epoch': row_end['EPOCH'],
            # Input Features (Normalized SGP4 state)
            'input_rx': r_pred[0], 'input_ry': r_pred[1], 'input_rz': r_pred[2],
            'input_vx': v_pred[0], 'input_vy': v_pred[1], 'input_vz': v_pred[2],
            'bstar': row_start['BSTAR'],
            'ndot': row_start['MEAN_MOTION_DOT'],
            'dt_minutes': time_diff_min,
            # Targets (The Error to learn)
            'err_rx': diff_r[0], 'err_ry': diff_r[1], 'err_rz': diff_r[2],
            'err_vx': diff_v[0], 'err_vy': diff_v[1], 'err_vz': diff_v[2]
        })

    # Save to CSV
    result_df = pd.DataFrame(training_data)
    out_name = "training_residuals.csv"
    result_df.to_csv(out_name, index=False)
    
    print(f"[+] Generated {len(result_df)} training samples.")
    print(f"[+] Saved to {out_name}")
    print("\nSample Data (First 5 rows):")
    print(result_df[['dt_minutes', 'err_rx', 'err_ry', 'err_rz']].head())



calculate_residuals("history_50803_2022-01-01_2025-12-17.csv", lookahead_hours=24)


class OrbitResidualDataset(Dataset):
    def __init__(self, csv_file):
        df = pd.read_csv(csv_file)

        # Nondimensionalize Inputs
        # Position: r / R_REF
        self.r_sgp4 = torch.tensor(
            df[['input_rx','input_ry','input_rz']].values / R_REF,
            dtype=torch.float32
        )

        # Velocity: v / V_REF
        self.v_sgp4 = torch.tensor(
            df[['input_vx','input_vy','input_vz']].values / V_REF,
            dtype=torch.float32
        )

        # Drag: Scale to order ~1
        # BSTAR is usually 1e-5, NDOT is small. 
        self.drag = torch.tensor(
            df[['bstar','ndot']].values,
            dtype=torch.float32
        )
        self.drag[:, 0] *= 1e5 # Scale BSTAR
        self.drag[:, 1] *= 1e5 # Scale NDOT (approx)

        # Time: t / T_REF
        # dt_minutes -> seconds -> nondimensional
        self.t = torch.tensor(
            (df['dt_minutes'].values * 60.0) / T_REF,
            dtype=torch.float32
        ).unsqueeze(1)

        # Targets (Residuals): Normalized same as state
        self.delta_r = torch.tensor(
            df[['err_rx','err_ry','err_rz']].values / R_REF,
            dtype=torch.float32
        )

        self.delta_v = torch.tensor(
            df[['err_vx','err_vy','err_vz']].values / V_REF,
            dtype=torch.float32
        )

    def __len__(self):
        return len(self.t)

    def __getitem__(self, idx):
        return (
            self.r_sgp4[idx],
            self.v_sgp4[idx],
            self.drag[idx],
            self.t[idx].requires_grad_(True),
            self.delta_r[idx],
            self.delta_v[idx],
        )


class ResidualPINN(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(3+3+2+1, 128),
            nn.SiLU(),
            nn.Linear(128, 128),
            nn.SiLU(),
            nn.Linear(128, 128), # Added depth
            nn.SiLU(),
            nn.Linear(128, 6)
        )

    def forward(self, r, v, drag, t):
        x = torch.cat([r, v, drag, t], dim=1)
        out = self.net(x)
        # Output is already in nondimensional units (order 1e-3 to 1e-2)
        # No aggressive scaling needed if inputs are normalized
        return out[:, :3], out[:, 3:]


def accel_j2(r):
    x,y,z = r[:,0], r[:,1], r[:,2]
    r_norm = torch.norm(r, dim=1)

    factor = 1.5 * J2 * MU_EARTH * R_EARTH**2 / r_norm**5
    zx = z / r_norm

    ax = factor * x * (5*zx*zx - 1)
    ay = factor * y * (5*zx*zx - 1)
    az = factor * z * (5*zx*zx - 3)

    return torch.stack([ax, ay, az], dim=1)

def accel_drag(r, v, bstar):
    rho0 = 3.614e-13
    H = 88.667
    alt = torch.norm(r, dim=1) - R_EARTH
    rho = rho0 * torch.exp(-alt / H)

    v_rel = v
    v_norm = torch.norm(v_rel, dim=1, keepdim=True)

    return -bstar[:,0:1] * rho.unsqueeze(1) * v_norm * v_rel


def physics_loss(r_sgp4_nd, v_sgp4_nd, delta_r_nd, delta_v_nd, drag_scaled, t_nd):
    """
    Calculates physics loss in Nondimensional Space.
    """
    # 1. Compute Derivatives wrt Nondimensional Time
    d_dr_dt = grad(delta_r_nd.sum(), t_nd, create_graph=True)[0]
    d_dv_dt = grad(delta_v_nd.sum(), t_nd, create_graph=True)[0]

    # 2. Reconstruct Full State (Nondimensional)
    r_nd = r_sgp4_nd + delta_r_nd
    v_nd = v_sgp4_nd + delta_v_nd

    # 3. Unscale to Dimensional Units for Physics Calculation
    r_dim = r_nd * R_REF
    v_dim = v_nd * V_REF
    
    # Unscale drag parameters
    bstar = drag_scaled[:, 0:1] / 1e5
    # ndot = drag_scaled[:, 1:2] / 1e5 # Not used in simple drag model

    # 4. Compute Accelerations (Dimensional: km/s^2)
    # Gravity
    r_norm = torch.norm(r_dim, dim=1, keepdim=True)
    a_grav = -MU_EARTH * r_dim / r_norm**3
    
    # Perturbations
    a_j2 = accel_j2(r_dim)
    a_drag = accel_drag(r_dim, v_dim, bstar)
    
    a_true_dim = a_grav + a_j2 + a_drag
    
    # SGP4 Baseline Acceleration (Point Mass Gravity)
    # Note: SGP4 includes J2/J3 etc internally, but for the residual 
    # we compare against the "bulk" force or the full force difference.
    # Here we assume SGP4 captures Point Mass + J2 approx.
    # But to be safe, we define the "Physics Deficit" as:
    # d(v_res)/dt = a_true - a_sgp4_dynamics
    # If we assume SGP4 ~ Point Mass for the baseline derivative:
    a_sgp4_dim = -MU_EARTH * (r_sgp4_nd * R_REF) / torch.norm(r_sgp4_nd * R_REF, dim=1, keepdim=True)**3
    
    # 5. Scale Acceleration Difference to Nondimensional
    # a_nd = a_dim / ACC_REF
    a_diff_dim = a_true_dim - a_sgp4_dim
    a_diff_nd = a_diff_dim / ACC_REF

    # 6. Loss Terms
    # Kinematic: d(r)/dt = v  =>  d(dr)/dt = dv
    L_kin = ((d_dr_dt - delta_v_nd)**2).mean()
    
    # Dynamic: d(v)/dt = a  =>  d(dv)/dt = a_true - a_sgp4
    # Fix #2: Constrain changes, not absolutes. 
    # The equation holds, but we expect small residuals.
    L_dyn = ((d_dv_dt - a_diff_nd)**2).mean()

    return L_kin, L_dyn


def rollout_loss(model, batch, steps=5, dt_seconds=60.0):
    r_nd, v_nd, drag, t_nd, dr_true_nd, dv_true_nd = batch
    loss = 0.0
    
    # Nondimensional dt
    dt_nd = dt_seconds / T_REF

    r_curr = r_nd
    v_curr = v_nd
    
    # We want to enforce that the INTEGRATED path matches the data
    # But we only have data at t=0 and t=target.
    # This rollout is a "consistency" check:
    # If we step forward, do we stay on a valid trajectory?
    # Since we don't have intermediate labels, we can only check 
    # if the physics holds (via physics loss at steps) OR 
    # if we reach the target (if steps*dt = target_time).
    #
    # User's original code compared against (r + dr_true) at every step.
    # But dr_true is only valid for the specific dt in the dataset row.
    #
    # FIX: We will just enforce smoothness/physics at rollout steps
    # OR, if we assume the error grows linearly (bad assumption), we can't compare.
    #
    # Better interpretation of User's request:
    # "Rollout loss... enforces trajectory consistency"
    # We will enforce that:
    # r_next = r_curr + v_curr * dt
    # v_next = v_curr + a * dt
    # And the network's predictions at t+dt match this integration.
    
    # Actually, the user's code was:
    # loss += ((r_pred - (r + dr_true))**2).mean()
    # This implies comparing to the FINAL target at every step? No, that's wrong.
    #
    # Let's implement a "Physics Rollout":
    # Integrate the state using the PINN-corrected dynamics
    # and ensure it doesn't diverge wildly?
    #
    # Re-reading User's code:
    # "loss += ((r_pred - (r + dr_true))**2).mean()"
    # This looks like it assumes the target error is constant? Or valid for all t?
    #
    # Let's implement a "Smoothness" rollout as requested in Fix #4.
    # L_smooth = || Δv(t+Δt) - Δv(t) ||²
    
    for k in range(1, steps + 1):
        t_next = t_nd + k * dt_nd
        
        # Predict at next step
        delta_r_next, delta_v_next = model(r_nd, v_nd, drag, t_next)
        
        # Predict at current (or previous) step
        t_prev = t_nd + (k-1) * dt_nd
        delta_r_prev, delta_v_prev = model(r_nd, v_nd, drag, t_prev)
        
        # Smoothness: Correction shouldn't jump
        loss += ((delta_v_next - delta_v_prev)**2).mean()
        loss += ((delta_r_next - delta_r_prev)**2).mean()

    return loss / steps


# Check for GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Load Data
dataset = OrbitResidualDataset("training_residuals.csv")
loader = DataLoader(dataset, batch_size=64, shuffle=True)

model = ResidualPINN().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# --- CURRICULUM LEARNING SETUP ---
# Fix #3: Staged Training
STAGES = [
    {"epochs": 50, "λ_phys": 0.0,   "λ_roll": 0.0,   "lr": 1e-3, "desc": "Stage 1: Data Fitting"},
    {"epochs": 30, "λ_phys": 1e-3,  "λ_roll": 0.0,   "lr": 5e-4, "desc": "Stage 2: Weak Physics"},
    {"epochs": 20, "λ_phys": 1e-2,  "λ_roll": 1e-2,  "lr": 1e-4, "desc": "Stage 3: Physics + Rollout"}
]

loss_history = []

print("Starting Curriculum Training...")

current_epoch = 0
for stage in STAGES:
    print(f"\n--- {stage['desc']} ---")
    print(f"Params: λ_phys={stage['λ_phys']}, λ_roll={stage['λ_roll']}, LR={stage['lr']}")
    
    # Update LR
    for param_group in optimizer.param_groups:
        param_group['lr'] = stage['lr']
        
    for epoch in range(stage['epochs']):
        model.train()
        total_loss = 0
        
        for batch in loader:
            batch = [b.to(device) for b in batch]
            r, v, drag, t, dr, dv = batch

            # Forward
            delta_r, delta_v = model(r, v, drag, t)

            # 1. Data Loss (MSE)
            L_data = ((delta_r - dr)**2).mean() + ((delta_v - dv)**2).mean()
            
            # 2. Physics Loss
            if stage['λ_phys'] > 0:
                L_kin, L_dyn = physics_loss(r, v, delta_r, delta_v, drag, t)
                L_phys = L_kin + L_dyn
            else:
                L_phys = torch.tensor(0.0).to(device)

            # 3. Rollout/Smoothness Loss
            if stage['λ_roll'] > 0:
                L_roll = rollout_loss(model, batch, steps=3, dt_seconds=60.0)
            else:
                L_roll = torch.tensor(0.0).to(device)

            # Total Loss
            loss = L_data + stage['λ_phys'] * L_phys + stage['λ_roll'] * L_roll

            optimizer.zero_grad()
            loss.backward()
            # Gradient Clipping (Fix #4: Stability)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            total_loss += loss.item()
        
        avg_loss = total_loss / len(loader)
        loss_history.append(avg_loss)
        
        if (epoch+1) % 10 == 0:
            print(f"Epoch {epoch+1}/{stage['epochs']} | Loss: {avg_loss:.6f}")

print("Training complete.")

# Save Model
torch.save(model.state_dict(), "orbit_error_model_pinn.pth")
print("Model saved.")


# Evaluation
model.eval()
with torch.no_grad():
    # Get all data
    r, v, drag, t, dr, dv = dataset[:]
    r, v, drag, t = r.to(device), v.to(device), drag.to(device), t.to(device)
    
    # Predict (Nondimensional)
    delta_r_pred_nd, delta_v_pred_nd = model(r, v, drag, t)
    
    # Unscale to km and km/s
    delta_r_pred = delta_r_pred_nd.cpu() * R_REF
    dr_true = dr * R_REF
    
    # Calculate Errors
    pos_error_sgp4 = torch.norm(dr_true, dim=1).numpy()
    pos_error_pinn = torch.norm(dr_true - delta_r_pred, dim=1).numpy()
    
    # Time in hours
    time_hours = (t.cpu().numpy().flatten() * T_REF) / 3600.0

    # Plot
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.scatter(time_hours, pos_error_sgp4, label="SGP4 Error", alpha=0.3, s=10, color='red')
    plt.scatter(time_hours, pos_error_pinn, label="PINN Error", alpha=0.3, s=10, color='blue')
    plt.xlabel("Time (hours)")
    plt.ylabel("Position Error (km)")
    plt.title("Error vs Time")
    plt.legend()
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    plt.plot(loss_history)
    plt.title("Training Loss (Curriculum)")
    plt.xlabel("Epochs")
    plt.yscale('log')
    plt.grid(True)
    
    plt.tight_layout()
    plt.show()


# Ablation Study Placeholder
print("Ablation Study:")
print("1. No Physics: Train with L_data only.")
print("2. Physics Only: Train with L_data + L_physics.")
print("3. Physics + Rollout: Train with all losses (Current Model).")

rms_sgp4 = np.sqrt(np.mean(pos_error_sgp4**2))
rms_pinn = np.sqrt(np.mean(pos_error_pinn**2))
max_sgp4 = np.max(pos_error_sgp4)
max_pinn = np.max(pos_error_pinn)

print(f"RMS Error (SGP4): {rms_sgp4:.4f} km")
print(f"RMS Error (PINN): {rms_pinn:.4f} km")
print(f"Max Error (SGP4): {max_sgp4:.4f} km")
print(f"Max Error (PINN): {max_pinn:.4f} km")



