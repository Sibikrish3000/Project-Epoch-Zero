get_ipython().getoutput("pip install skyfield sgp4 torchdiffeq spacetrack joblib seaborn")


import pandas as pd
from spacetrack import SpaceTrackClient
import numpy as np
from sgp4.api import Satrec, WGS72
import json
from datetime import datetime, timezone
print("âœ… Libraries imported successfully!")
print(f"NumPy version: {np.__version__}")
print(f"Pandas version: {pd.__version__}")


def download_historical_tles(norad_id, start_date, end_date, username, password):
    """
    Downloads TLE history for a specific NORAD ID.

    Args:
        norad_id (int): Satellite Catalog Number (e.g., 50803)
        start_date (str): 'YYYY-MM-DD'
        end_date (str): 'YYYY-MM-DD'
        username (str): Space-Track login email
        password (str): Space-Track login password
    """
    print(f"[-] Authenticating as {username}...")
    st = SpaceTrackClient(identity=username, password=password)

    print(f"[-] Fetching TLEs for ID {norad_id} from {start_date} to {end_date}...")

    try:
        # Query: Filter by NORAD ID and Epoch Range
        # orderby='EPOCH desc' ensures we get data in order
        tles = st.tle(
            norad_cat_id=norad_id,
            epoch=f"{start_date}--{end_date}",
            orderby='EPOCH',
            format='json'
        )

        if not tles:
            print("No TLEs found for this range.")
            return

        # Convert to Pandas DataFrame
        df = pd.DataFrame(json.loads(tles))

        # Select critical columns for SGP4 and PINN training
        cols = [
            'EPOCH', 'NORAD_CAT_ID', 'MEAN_MOTION', 'ECCENTRICITY',
            'INCLINATION', 'RA_OF_ASC_NODE', 'ARG_OF_PERICENTER',
            'MEAN_ANOMALY', 'MEAN_MOTION_DOT', 'BSTAR',
            'TLE_LINE1', 'TLE_LINE2'
        ]
        df = df[cols]

        # Type cleaning
        df['EPOCH'] = pd.to_datetime(df['EPOCH'])
        df = df.sort_values('EPOCH')

        # Save
        filename = f"history_{norad_id}_{start_date}_{end_date}.csv"
        df.to_csv(filename, index=False)
        print(f"[+] Success. Saved {len(df)} TLEs to {filename}")
        print(df[['EPOCH', 'MEAN_MOTION', 'MEAN_MOTION_DOT']].head())

    except Exception as e:
        print(f"[!] API Error: {e}")




# Configuration
TARGET_ID = 50803  # Starlink-3321
START = "2022-01-01"
END = "2025-12-17"
username = 'sibikrish3000@gmail.com'
password = 'Z!2V0XBU1arMv8q'

download_historical_tles(TARGET_ID, START, END, username, password)


def calculate_residuals(csv_path, lookahead_hours=24):
    """
    Generates training pairs (Input State -> Output Error).

    Args:
        csv_path (str): Path to the historical TLE CSV.
        lookahead_hours (float): How far into the future to compare (the gap).
    """
    print(f"[-] Loading {csv_path}...")
    df = pd.read_csv(csv_path)
    df['EPOCH'] = pd.to_datetime(df['EPOCH'])
    df = df.sort_values('EPOCH').reset_index(drop=True)

    training_data = []

    print(f"[-] Generating residuals with {lookahead_hours}h lookahead...")

    # Iterate through the timeline
    for i in range(len(df)):
        # 1. The "Anchor" TLE (Input)
        row_start = df.iloc[i]
        sat_start = Satrec.twoline2rv(row_start['TLE_LINE1'], row_start['TLE_LINE2'], WGS72)

        # 2. Find a "Target" TLE close to the lookahead time
        # We search for a TLE that was created roughly 'lookahead_hours' later
        target_time = row_start['EPOCH'] + pd.Timedelta(hours=lookahead_hours)

        # Filter for rows close to target_time (within +/- 2 hours tolerance)
        # Using searchsorted for speed on sorted index is faster, but simple masking is clearer here
        mask = (df['EPOCH'] > target_time - pd.Timedelta(hours=2)) & \
               (df['EPOCH'] < target_time + pd.Timedelta(hours=2))

        candidates = df[mask]

        if candidates.empty:
            continue

        # Pick the closest candidate
        row_end = candidates.iloc[0]
        sat_end = Satrec.twoline2rv(row_end['TLE_LINE1'], row_end['TLE_LINE2'], WGS72)

        # 3. Propagate "Anchor" to "Target" time
        # Calculate time difference in minutes
        time_diff_min = (row_end['EPOCH'] - row_start['EPOCH']).total_seconds() / 60.0

        # SGP4 Propagation (Prediction)
        e, r_pred, v_pred = sat_start.sgp4_tsince(time_diff_min)

        if e != 0: continue # Skip errors

        # 4. Get "Target" True State at its own Epoch
        # We propagate the Target TLE by 0 minutes to get its initial state
        _, r_true, v_true = sat_end.sgp4_tsince(0.0)

        # 5. Compute Residuals (The "Label" for the Neural Net)
        # Difference in km and km/s
        diff_r = np.array(r_true) - np.array(r_pred)
        diff_v = np.array(v_true) - np.array(v_pred)

        # 6. Store Feature Vector
        # Features: [rx, ry, rz, vx, vy, vz, BSTAR, ndot, time_delta]
        # Targets:  [drx, dry, drz, dvx, dvy, dvz]
        training_data.append({
            'start_epoch': row_start['EPOCH'],
            'end_epoch': row_end['EPOCH'],
            # Input Features (Normalized SGP4 state)
            'input_rx': r_pred[0], 'input_ry': r_pred[1], 'input_rz': r_pred[2],
            'input_vx': v_pred[0], 'input_vy': v_pred[1], 'input_vz': v_pred[2],
            'bstar': row_start['BSTAR'],
            'ndot': row_start['MEAN_MOTION_DOT'],
            'dt_minutes': time_diff_min,
            # Targets (The Error to learn)
            'target_error_rx': diff_r[0], 'target_error_ry': diff_r[1], 'target_error_rz': diff_r[2],
            'target_error_vx': diff_v[0], 'target_error_vy': diff_v[1], 'target_error_vz': diff_v[2]
        })

    # Save to CSV
    result_df = pd.DataFrame(training_data)
    out_name = "training_residuals.csv"
    result_df.to_csv(out_name, index=False)

    print(f"[+] Generated {len(result_df)} training samples.")
    print(f"[+] Saved to {out_name}")
    print("\nSample Data (First 5 rows):")
    print(result_df[['dt_minutes', 'target_error_rx', 'target_error_ry', 'target_error_rz']].head())



calculate_residuals("history_50803_2022-01-01_2025-12-17.csv", lookahead_hours=24)


import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import joblib
import os

# --- CONFIGURATION ---
BATCH_SIZE = 64
LEARNING_RATE = 0.001
EPOCHS = 300
DATA_PATH = "training_residuals.csv"
MODEL_PATH = "orbit_error_model.pth"
SCALER_X_PATH = "scaler_X.pkl"
SCALER_Y_PATH = "scaler_Y.pkl"

# --- 1. DATASET CLASS ---
class OrbitResidualDataset(Dataset):
    def __init__(self, csv_file):
        print(f"[-] Loading dataset: {csv_file}")
        df = pd.read_csv(csv_file)

        # FEATURES (Inputs):
        # SGP4 State [6] + Drag Terms [2] + Time Delta [1] = 9 Inputs
        self.X_raw = df[[
            'input_rx', 'input_ry', 'input_rz',
            'input_vx', 'input_vy', 'input_vz',
            'bstar', 'ndot', 'dt_minutes'
        ]].values.astype(np.float32)

        # TARGETS (Labels):
        # The Error (Truth - SGP4) = 6 Outputs
        self.y_raw = df[[
            'target_error_rx', 'target_error_ry', 'target_error_rz',
            'target_error_vx', 'target_error_vy', 'target_error_vz'
        ]].values.astype(np.float32)

        # --- NORMALIZATION (CRITICAL) ---
        # Scale inputs to mean=0, var=1 to help the optimizer
        self.scaler_X = StandardScaler()
        self.scaler_y = StandardScaler()

        self.X = self.scaler_X.fit_transform(self.X_raw)
        self.y = self.scaler_y.fit_transform(self.y_raw)

        # Save scalers for the Inference stage
        joblib.dump(self.scaler_X, SCALER_X_PATH)
        joblib.dump(self.scaler_y, SCALER_Y_PATH)
        print("[-] Scalers computed and saved.")

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return torch.tensor(self.X[idx]), torch.tensor(self.y[idx])

# --- 2. MODEL ARCHITECTURE ---
class ResidualPINN(nn.Module):
    def __init__(self):
        super(ResidualPINN, self).__init__()

        # Input: 9 | Output: 6
        # Using Tanh activations for smooth, physics-compatible gradients
        self.net = nn.Sequential(
            nn.Linear(9, 128),
            nn.Tanh(),
            nn.Linear(128, 256),
            nn.Tanh(),
            nn.Linear(256, 128),
            nn.Tanh(),
            nn.Linear(128, 6)
        )

    def forward(self, x):
        return self.net(x)

# --- 3. TRAINING LOOP ---
def train_model():
    # check for GPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[-] Training on {device}")

    # Load Data
    dataset = OrbitResidualDataset(DATA_PATH)
    # Split into Train/Test (80/20)
    train_size = int(0.8 * len(dataset))
    test_size = len(dataset) - train_size
    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # Initialize Network
    model = ResidualPINN().to(device)
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.MSELoss() # Mean Squared Error

    print(f"[-] Starting training for {EPOCHS} epochs...")

    for epoch in range(EPOCHS):
        model.train()
        train_loss = 0.0

        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)

            optimizer.zero_grad()
            predictions = model(inputs)
            loss = criterion(predictions, targets)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

        # Validation Step
        if (epoch + 1) % 10 == 0:
            model.eval()
            val_loss = 0.0
            with torch.no_grad():
                for inputs, targets in test_loader:
                    inputs, targets = inputs.to(device), targets.to(device)
                    preds = model(inputs)
                    loss = criterion(preds, targets)
                    val_loss += loss.item()

            avg_train = train_loss / len(train_loader)
            avg_val = val_loss / len(test_loader)
            print(f"Epoch [{epoch+1}/{EPOCHS}] | Train Loss: {avg_train:.5f} | Val Loss: {avg_val:.5f}")

    # --- SAVE ---
    torch.save(model.state_dict(), MODEL_PATH)
    print(f"[+] Model saved to {MODEL_PATH}")



if os.path.exists(DATA_PATH):
    train_model()
else:
    print(f"[!] Error: {DATA_PATH} not found. Run the preprocessing script first.")


import torch
import numpy as np
import joblib
from sgp4.api import Satrec, WGS72
import pandas as pd

# --- CONFIGURATION ---
MODEL_PATH = "orbit_error_model.pth"
SCALER_X_PATH = "scaler_X.pkl"
SCALER_Y_PATH = "scaler_Y.pkl"

# --- 1. DEFINE MODEL ARCHITECTURE (Must match training exactly) ---
class ResidualPINN(torch.nn.Module):
    def __init__(self):
        super(ResidualPINN, self).__init__()
        self.net = torch.nn.Sequential(
            torch.nn.Linear(9, 128),
            torch.nn.Tanh(),
            torch.nn.Linear(128, 256),
            torch.nn.Tanh(),
            torch.nn.Linear(256, 128),
            torch.nn.Tanh(),
            torch.nn.Linear(128, 6)
        )
    def forward(self, x): return self.net(x)

# --- 2. INFERENCE ENGINE ---
class OrbitCorrector:
    def __init__(self):
        # Load Resources
        self.device = torch.device("cpu") # Inference is fast on CPU
        self.model = ResidualPINN()

        try:
            self.model.load_state_dict(torch.load(MODEL_PATH, map_location=self.device))
            self.model.eval() # Set to evaluation mode
            self.scaler_X = joblib.load(SCALER_X_PATH)
            self.scaler_Y = joblib.load(SCALER_Y_PATH)
            print("[+] Model and Scalers loaded successfully.")
        except FileNotFoundError:
            print("[!] Error: Model files not found. Train the model first.")
            exit()

    def get_precise_orbit(self, line1, line2, target_epoch_iso):
        """
        Returns:
            dict: {
                'sgp4_pos': [x,y,z],
                'ai_correction_pos': [dx, dy, dz],
                'final_pos': [x,y,z]
            }
        """
        # A. Initialize SGP4
        satellite = Satrec.twoline2rv(line1, line2, WGS72)

        # Calculate Time Delta (minutes)
        # epoch_start = pd.to_datetime(satellite.epoch, unit='D', origin='2022-01-01') # Approx for logic
        # Better: use jdsatepoch directly
        # FIX: Calculate TLE Epoch from Julian Date components
        # JD of Unix Epoch (1970-01-01) = 2440587.5
        jd_full = satellite.jdsatepoch + satellite.jdsatepochF
        ts_start = pd.Timestamp(jd_full - 2440587.5, unit='D')

        ts_target = pd.to_datetime(target_epoch_iso)

        dt_minutes = (ts_target - ts_start).total_seconds() / 60.0

        # B. Get SGP4 Baseline
        e, r_sgp4, v_sgp4 = satellite.sgp4_tsince(dt_minutes)
        if e != 0: return None

        # C. Prepare Input Vector for AI
        # [rx, ry, rz, vx, vy, vz, bstar, ndot, dt]
        features = np.array([
            r_sgp4[0], r_sgp4[1], r_sgp4[2],
            v_sgp4[0], v_sgp4[1], v_sgp4[2],
            satellite.bstar,
            satellite.ndot,
            dt_minutes
        ]).reshape(1, -1)

        # D. Normalize Input
        features_scaled = self.scaler_X.transform(features)
        tensor_in = torch.tensor(features_scaled, dtype=torch.float32)

        # E. AI Prediction
        with torch.no_grad():
            prediction_scaled = self.model(tensor_in)

        # F. Denormalize Output (The Correction)
        correction = self.scaler_Y.inverse_transform(prediction_scaled.numpy())[0]
        # correction = [drx, dry, drz, dvx, dvy, dvz]

        # G. Apply Correction
        r_final = np.array(r_sgp4) + correction[0:3]
        v_final = np.array(v_sgp4) + correction[3:6]

        return {
            "dt_hours": dt_minutes / 60,
            "sgp4_r": r_sgp4,
            "ai_correction_r": correction[0:3],
            "final_r": r_final
        }




corrector = OrbitCorrector()

# Test Data (Starlink-3321)
tle1 = "1 50803U 22001A   22011.83334491 -.00918374  26886-3 -20449-2 0  9990"
tle2 = "2 50803  53.2176 175.5863 0053823 179.7175 211.9048 15.94459142  2073"

# Target: 24 hours later
target_time = "2022-01-12 20:00:00"

result = corrector.get_precise_orbit(tle1, tle2, target_time)

if result:
    print(f"\n--- Prediction (+{result['dt_hours']:.1f} hours) ---")
    print(f"SGP4 Position (km):   {result['sgp4_r']}")
    print(f"AI Correction (km):   {result['ai_correction_r']}")
    print(f"Final Position (km):  {result['final_r']}")

    err_mag = np.linalg.norm(result['ai_correction_r'])
    print(f"Estimated SGP4 Error: {err_mag:.3f} km")


import torch
import numpy as np
import pandas as pd
import joblib
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns

# --- CONFIG ---
MODEL_PATH = "orbit_error_model.pth"
SCALER_X_PATH = "scaler_X.pkl"
SCALER_Y_PATH = "scaler_Y.pkl"
DATA_PATH = "training_residuals.csv"

# 1. DEFINE CLASS GLOBALLY (Must match training exactly)
class ResidualPINN(torch.nn.Module):
    def __init__(self):
        super(ResidualPINN, self).__init__()
        self.net = torch.nn.Sequential(
            torch.nn.Linear(9, 128), torch.nn.Tanh(),
            torch.nn.Linear(128, 256), torch.nn.Tanh(),
            torch.nn.Linear(256, 128), torch.nn.Tanh(),
            torch.nn.Linear(128, 6)
        )
    def forward(self, x): return self.net(x)

def evaluate_model():
    # 2. Load Resources Correctly
    device = torch.device("cpu")

    # --- FIX STARTS HERE ---
    model = ResidualPINN() # A. Create the empty structure
    state_dict = torch.load(MODEL_PATH, map_location=device) # B. Load weights from file
    model.load_state_dict(state_dict) # C. Pour weights into structure
    model.eval() # D. Set to evaluation mode
    # --- FIX ENDS HERE ---

    scaler_X = joblib.load(SCALER_X_PATH)
    scaler_Y = joblib.load(SCALER_Y_PATH)

    # 3. Load Test Data (Last 20%)
    df = pd.read_csv(DATA_PATH)
    split_idx = int(0.8 * len(df))
    test_df = df.iloc[split_idx:].copy()

    print(f"[-] Testing on {len(test_df)} unseen samples...")

    # 4. Prepare Inputs
    X_cols = ['input_rx', 'input_ry', 'input_rz', 'input_vx', 'input_vy', 'input_vz', 'bstar', 'ndot', 'dt_minutes']
    Y_cols = ['target_error_rx', 'target_error_ry', 'target_error_rz', 'target_error_vx', 'target_error_vy', 'target_error_vz']

    X_test = test_df[X_cols].values
    y_true = test_df[Y_cols].values

    # 5. Model Prediction
    X_scaled = scaler_X.transform(X_test)
    with torch.no_grad():
        preds_scaled = model(torch.tensor(X_scaled, dtype=torch.float32)).numpy()

    y_pred = scaler_Y.inverse_transform(preds_scaled)

    # 6. Calc Metrics
    final_error = y_true - y_pred

    def get_magnitude(vec_array):
        return np.linalg.norm(vec_array[:, 0:3], axis=1)

    orig_err_km = get_magnitude(y_true)
    final_err_km = get_magnitude(final_error)

    print("\n--- RESULTS (Position Accuracy) ---")
    print(f"SGP4 Mean Error:      {np.mean(orig_err_km):.3f} km")
    print(f"Hybrid Mean Error:    {np.mean(final_err_km):.3f} km")
    print(f"Error Reduction:      {((1 - np.mean(final_err_km)/np.mean(orig_err_km))*100):.1f}%")

    print(f"\nSGP4 RMSE:            {np.sqrt(mean_squared_error(y_true, np.zeros_like(y_true))):.3f}")
    print(f"Hybrid RMSE:          {np.sqrt(mean_squared_error(y_true, y_pred)):.3f}")

    return orig_err_km, final_err_km



orig, final = evaluate_model()
np.save("err_sgp4.npy", orig)
np.save("err_hybrid.npy", final)


import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Load data
err_sgp4 = np.load("err_sgp4.npy")
err_hybrid = np.load("err_hybrid.npy")

# Setup Plot
plt.figure(figsize=(12, 6))
sns.set_style("whitegrid")

# 1. Histogram Comparison
plt.subplot(1, 2, 1)
sns.histplot(err_sgp4, color="red", label="Standard SGP4", kde=True, alpha=0.3)
sns.histplot(err_hybrid, color="blue", label="Hybrid PINN", kde=True, alpha=0.3)
plt.title("Error Distribution (24h Prediction)")
plt.xlabel("Position Error (km)")
plt.ylabel("Count")
plt.legend()

# 2. Box Plot (To see outliers)
plt.subplot(1, 2, 2)
data = [err_sgp4, err_hybrid]
plt.boxplot(data, labels=['SGP4', 'Hybrid PINN'], patch_artist=True)
plt.title("Error Consistency")
plt.ylabel("Position Error (km)")

plt.tight_layout()
plt.show()


import torch
import numpy as np
import joblib
from sgp4.api import Satrec, WGS72
import pandas as pd
import math

# --- CONFIG ---
MODEL_PATH = "orbit_error_model.pth"
SCALER_X_PATH = "scaler_X.pkl"
SCALER_Y_PATH = "scaler_Y.pkl"
SIGMA_ERROR = 9.509 # km (Derived from your Validation Mean Error)

# --- MODEL DEFINITION ---
class ResidualPINN(torch.nn.Module):
    def __init__(self):
        super(ResidualPINN, self).__init__()
        self.net = torch.nn.Sequential(
            torch.nn.Linear(9, 128), torch.nn.Tanh(),
            torch.nn.Linear(128, 256), torch.nn.Tanh(),
            torch.nn.Linear(256, 128), torch.nn.Tanh(),
            torch.nn.Linear(128, 6)
        )
    def forward(self, x): return self.net(x)

class ConjunctionAssessor:
    def __init__(self):
        self.device = torch.device("cpu")
        self.model = ResidualPINN()
        self.model.load_state_dict(torch.load(MODEL_PATH, map_location=self.device))
        self.model.eval()
        self.scaler_X = joblib.load(SCALER_X_PATH)
        self.scaler_Y = joblib.load(SCALER_Y_PATH)

    def get_corrected_state(self, tle1, tle2, target_time):
        # 1. SGP4 Propagate
        sat = Satrec.twoline2rv(tle1, tle2, WGS72)
        jd_full = sat.jdsatepoch + sat.jdsatepochF
        ts_start = pd.Timestamp(jd_full - 2440587.5, unit='D')
        ts_target = pd.to_datetime(target_time)
        dt_minutes = (ts_target - ts_start).total_seconds() / 60.0

        e, r, v = sat.sgp4_tsince(dt_minutes)
        if e != 0: return None, None

        # 2. AI Correct
        features = np.array([r[0], r[1], r[2], v[0], v[1], v[2], sat.bstar, sat.ndot, dt_minutes]).reshape(1, -1)
        feat_scaled = self.scaler_X.transform(features)
        with torch.no_grad():
            pred_scaled = self.model(torch.tensor(feat_scaled, dtype=torch.float32))
        correction = self.scaler_Y.inverse_transform(pred_scaled.numpy())[0]

        r_final = np.array(r) + correction[0:3]
        return np.array(r), r_final # Return (SGP4_Raw, AI_Corrected)

# --- EXECUTION ---
if __name__ == "__main__":
    assessor = ConjunctionAssessor()

    # 1. PRIMARY OBJECT (Starlink-3321)
    tle1_A = "1 50803U 22001A   22011.83334491 -.00918374  26886-3 -20449-2 0  9990"
    tle2_A = "2 50803  53.2176 175.5863 0053823 179.7175 211.9048 15.94459142  2073"

    # 2. SECONDARY OBJECT (Simulated Debris on collision course)
    # We slightly modify the TLE to create a "Close Approach" scenario
    tle1_B = "1 99999U 22001A   22011.83334491 -.00918374  26886-3 -20449-2 0  9990"
    tle2_B = "2 99999  53.2100 175.6000 0053823 179.7175 211.9048 15.94459142  2073"

    target_time = "2022-01-12 20:00:00" # 24 hours later

    # Get Positions
    raw_A, corr_A = assessor.get_corrected_state(tle1_A, tle2_A, target_time)
    raw_B, corr_B = assessor.get_corrected_state(tle1_B, tle2_B, target_time) # Assuming AI applies to debris too

    # 3. CALCULATE MISS DISTANCE
    miss_sgp4 = np.linalg.norm(raw_A - raw_B)
    miss_ai = np.linalg.norm(corr_A - corr_B)

    # 4. PROBABILITY (Isotropic Gaussian Approximation)
    # Risk Radius = Combined Hard Body Radius (0.01km) + 3-Sigma Uncertainty
    # We use your validation error (9.5 km) as 1-Sigma
    combined_sigma = np.sqrt(SIGMA_ERROR**2 + SIGMA_ERROR**2) # Variance sum
    mahalanobis_dist = miss_ai / combined_sigma
    collision_prob = math.exp(-0.5 * mahalanobis_dist**2) # Simplified 1D Gaussian peak

    print(f"\n--- CONJUNCTION ASSESSMENT ({target_time}) ---")
    print(f"Miss Distance (SGP4):   {miss_sgp4:.3f} km")
    print(f"Miss Distance (Hybrid): {miss_ai:.3f} km")
    print(f"Uncertainty (1-Sigma):  {SIGMA_ERROR:.3f} km")
    print(f"Collision Probability:  {collision_prob:.6f}")

    if miss_ai < (3 * combined_sigma):
        print("\n[!] ALERT: Object within 3-Sigma covariance ellipsoid.")
    else:
        print("\n[+] SAFE: Object outside risk volume.")



